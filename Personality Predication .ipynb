{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565e8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import array\n",
    "import pandas\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d851f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile=open('newfrequency300.csv', 'rt')\n",
    "csvReader=csv.reader(csvFile)\n",
    "mydict={row[1]: int(row[0]) for row in csvReader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dadd348",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "with open ('PJFinaltest.csv', 'rt') as f:\n",
    "\treader=csv.reader(f)\n",
    "\tcorpus=[rows[0] for rows in reader]\n",
    "\n",
    "with open ('PJFinaltest.csv', 'rt') as f:\n",
    "\tcsvReader1=csv.reader(f)\n",
    "\tfor rows in csvReader1:\n",
    "\t\ty.append([int(rows[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222a097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1322: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
    "x=vectorizer.fit_transform(corpus).toarray()\n",
    "result=np.append(x,y,axis=1)\n",
    "X=pandas.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988f8af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265610, 302)\n",
      "(66403, 302)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=GaussianNB()\n",
    "train = X.sample(frac=0.8, random_state=1)\n",
    "test=X.drop(train.index)\n",
    "y_train=train[301]\n",
    "y_test=test[301]\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "xtrain=train.drop(301,axis=1)\n",
    "xtest=test.drop(301,axis=1)\n",
    "model.fit(xtrain , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333d0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('BNIEFinal.sav', 'wb'))\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea46b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1322: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256000, 302)\n",
      "(64000, 302)\n"
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "with open ('TFFinaltest.csv', 'rt') as f:\n",
    "\treader=csv.reader(f)\n",
    "\tcorpus=[rows[0] for rows in reader]\n",
    "\n",
    "with open ('TFFinaltest.csv', 'rt') as f:\n",
    "\tcsvReader1=csv.reader(f)\n",
    "\tfor rows in csvReader1:\n",
    "\t\ty.append([int(rows[1])])\n",
    "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
    "x=vectorizer.fit_transform(corpus).toarray()\n",
    "result=np.append(x,y,axis=1)\n",
    "X=pandas.DataFrame(result)\n",
    "model=GaussianNB()\n",
    "train = X.sample(frac=0.8, random_state=1)\n",
    "test=X.drop(train.index)\n",
    "y_train=train[301]\n",
    "y_test=test[301]\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "xtrain=train.drop(301,axis=1)\n",
    "xtest=test.drop(301,axis=1)\n",
    "model.fit(xtrain,y_train)\n",
    "pickle.dump(model, open('BNTFFinal.sav', 'wb'))\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3b22d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1322: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188541, 302)\n",
      "(47135, 302)\n"
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "with open ('SNFinaltest.csv', 'rt') as f:\n",
    "\treader=csv.reader(f)\n",
    "\tcorpus=[rows[0] for rows in reader]\n",
    "\n",
    "with open ('SNFinaltest.csv', 'rt') as f:\n",
    "\tcsvReader1=csv.reader(f)\n",
    "\tfor rows in csvReader1:\n",
    "\t\ty.append([int(rows[1])])\n",
    "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
    "x=vectorizer.fit_transform(corpus).toarray()\n",
    "result=np.append(x,y,axis=1)\n",
    "X=pandas.DataFrame(result)\n",
    "model=GaussianNB()\n",
    "train = X.sample(frac=0.8, random_state=1)\n",
    "test=X.drop(train.index)\n",
    "y_train=train[301]\n",
    "y_test=test[301]\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "xtrain=train.drop(301,axis=1)\n",
    "xtest=test.drop(301,axis=1)\n",
    "model.fit(xtrain,y_train)\n",
    "pickle.dump(model, open('BNSNFinal.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c84ed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import tweepy\n",
    "import sys\n",
    "import os\n",
    "import nltk \n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import csv\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9753dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "ckey='BQDFBbn6oDUGiXnm0G2Vo2GMF'\n",
    "csecret='pB7fJ4OAn78D8tw3DsiOZDS2LDEh8a8inzMkLN233yNg8HJ9e4'\n",
    "atoken='1389940208678981646-jHQ6HejJ0m86GnpGBL8n0OBKx59PDX'\n",
    "asecret='2kQzbe9uMl70isghjDP1yKxTrxgcWk9tWqCkVAmImjCaA'\n",
    "auth=tweepy.OAuthHandler(ckey, csecret)\n",
    "auth.set_access_token(atoken, asecret)\n",
    "api=tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2800035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>',  # HTML tags\n",
    "    r'(?:@[\\w_]+)',  # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",  # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',  # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',  # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\",  # words with - and '\n",
    "    r'(?:[\\w_]+)',  # other words\n",
    "    r'(?:\\S)'  # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^' + emoticons_str + '$', re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a150cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "\n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ee01705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(s):\n",
    "\t#s=emoji_pattern.sub(r'', s) # no emoji\n",
    "\ts= unidecode(s)\n",
    "\tPOSTagger=preprocess(s)\n",
    "\t#print(POSTagger)\n",
    "\n",
    "\ttweet=' '.join(POSTagger)\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\tword_tokens = word_tokenize(tweet)\n",
    "\t#filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\tfiltered_sentence = []\n",
    "\tfor w in POSTagger:\n",
    "\t    if w not in stop_words:\n",
    "\t        filtered_sentence.append(w)\n",
    "\t#print(word_tokens)\n",
    "\t#print(filtered_sentence)\n",
    "\tstemmed_sentence=[]\n",
    "\tstemmer2 = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\tfor w in filtered_sentence:\n",
    "\t\tstemmed_sentence.append(stemmer2.stem(w))\n",
    "\t#print(stemmed_sentence)\n",
    "\n",
    "\ttemp = ' '.join(c for c in stemmed_sentence if c not in string.punctuation) \n",
    "\tpreProcessed=temp.split(\" \")\n",
    "\tfinal=[]\n",
    "\tfor i in preProcessed:\n",
    "\t\tif i not in final:\n",
    "\t\t\tif i.isdigit():\n",
    "\t\t\t\tpass\n",
    "\t\t\telse:\n",
    "\t\t\t\tif 'http' not in i:\n",
    "\t\t\t\t\tfinal.append(i)\n",
    "\ttemp1=' '.join(c for c in final)\n",
    "\t#print(preProcessed)\n",
    "\treturn temp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a44b9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Enter Twitter Account handle: govindaahuja\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected parameter: next\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tweepy.errors' has no attribute 'TweepError'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mgetTweets\u001b[1;34m(user)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m \ttweets\u001b[38;5;241m=\u001b[39m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_timeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscreen_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_rts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m status \u001b[38;5;129;01min\u001b[39;00m tweets:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tweepy\\api.py:33\u001b[0m, in \u001b[0;36mpagination.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tweepy\\api.py:46\u001b[0m, in \u001b[0;36mpayload.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_type\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tweepy\\api.py:571\u001b[0m, in \u001b[0;36mAPI.user_timeline\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;124;03m\"\"\"user_timeline(*, user_id, screen_name, since_id, count, max_id, \\\u001b[39;00m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;124;03m                 trim_user, exclude_replies, include_rts)\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03mhttps://developer.twitter.com/en/docs/twitter-api/v1/tweets/timelines/api-reference/get-statuses-user_timeline\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatuses/user_timeline\u001b[39m\u001b[38;5;124m'\u001b[39m, endpoint_parameters\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msince_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrim_user\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexclude_replies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_rts\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    575\u001b[0m     ), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    576\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tweepy\\api.py:257\u001b[0m, in \u001b[0;36mAPI.request\u001b[1;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(resp)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n",
      "\u001b[1;31mUnauthorized\u001b[0m: 401 Unauthorized\n89 - Invalid or expired token.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \tcsvFile\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     18\u001b[0m username\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease Enter Twitter Account handle: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mgetTweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     21\u001b[0m \tcsvReader\u001b[38;5;241m=\u001b[39mcsv\u001b[38;5;241m.\u001b[39mreader(f)\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mgetTweets\u001b[1;34m(user)\u001b[0m\n\u001b[0;32m     10\u001b[0m \t\t\t\ttw\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \t\t\tcsvWriter\u001b[38;5;241m.\u001b[39mwriterow([tw])\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mtweepy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTweepError\u001b[49m:\n\u001b[0;32m     13\u001b[0m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run the command on that user, Skipping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m csvFile\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tweepy.errors' has no attribute 'TweepError'"
     ]
    }
   ],
   "source": [
    "def getTweets(user):\n",
    "\tcsvFile = open('user.csv', 'a', newline='')\n",
    "\tcsvWriter = csv.writer(csvFile)\n",
    "\ttry:\n",
    "\t\tfor i in range(0,4):\n",
    "\t\t\ttweets=api.user_timeline(screen_name = user, count = 1000, include_rts=True, next = i)\n",
    "\t\t\tfor status in tweets:\n",
    "\t\t\t\ttw=preproc(status.text)\n",
    "\t\t\t\tif tw.find(\" \") == -1:\n",
    "\t\t\t\t\ttw=\"blank\"\n",
    "\t\t\t\tcsvWriter.writerow([tw])\n",
    "\texcept tweepy.errors.TweepError:\n",
    "\t\tprint(\"Failed to run the command on that user, Skipping...\")\n",
    "\tcsvFile.close()\n",
    "\n",
    "\n",
    "\n",
    "username=input(\"Please Enter Twitter Account handle: \")\n",
    "getTweets(username)\n",
    "with open('user.csv','rt') as f:\n",
    "\tcsvReader=csv.reader(f)\n",
    "\ttweetList=[rows[0] for rows in csvReader]\n",
    "with open('newfrequency300.csv','rt') as f:\n",
    "\tcsvReader=csv.reader(f)\n",
    "\tmydict={rows[1]: int(rows[0]) for rows in csvReader}\n",
    "\n",
    "vectorizer=TfidfVectorizer(vocabulary=mydict,min_df=1)\n",
    "x=vectorizer.fit_transform(tweetList).toarray()\n",
    "df=pd.DataFrame(x)\n",
    "\n",
    "\n",
    "model_IE = pickle.load(open(\"BNIEFinal.sav\", 'rb'))\n",
    "model_SN = pickle.load(open(\"BNSNFinal.sav\", 'rb'))\n",
    "model_TF = pickle.load(open('BNTFFinal.sav', 'rb'))\n",
    "model_PJ = pickle.load(open('BNPJFinal.sav', 'rb'))\n",
    "\n",
    "answer=[]\n",
    "IE=model_IE.predict(df)\n",
    "SN=model_SN.predict(df)\n",
    "TF=model_TF.predict(df)\n",
    "PJ=model_PJ.predict(df)\n",
    "\n",
    "\n",
    "b = Counter(IE)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1.0:\n",
    "\tanswer.append(\"I\")\n",
    "else:\n",
    "\tanswer.append(\"E\")\n",
    "\n",
    "b = Counter(SN)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1.0:\n",
    "\tanswer.append(\"S\")\n",
    "else:\n",
    "\tanswer.append(\"N\")\n",
    "\n",
    "b = Counter(TF)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1:\n",
    "\tanswer.append(\"T\")\n",
    "else:\n",
    "\tanswer.append(\"F\")\n",
    "\n",
    "b = Counter(PJ)\n",
    "value=b.most_common(1)\n",
    "print(value)\n",
    "if value[0][0] == 1:\n",
    "\tanswer.append(\"P\")\n",
    "else:\n",
    "\tanswer.append(\"J\")\n",
    "mbti=\"\".join(answer)\n",
    "print(mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e535dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60b199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fc501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
